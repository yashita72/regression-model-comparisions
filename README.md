ğŸ“Š Regression Models Comparison with Regularization
ğŸ“Œ Project Overview

This project focuses on a comparative analysis of regression models to understand the impact of:

feature scaling

polynomial feature expansion

regularization techniques (L1, L2)

The goal is not just prediction, but model interpretability and performance analysis, which is crucial in real-world machine learning applications.

ğŸ¯ Objectives

Implement and compare different regression models

Study the effect of overfitting and underfitting

Analyze coefficient shrinkage and feature selection

Understand the biasâ€“variance tradeoff

ğŸ“‚ Dataset

California Housing Dataset (from sklearn.datasets)

Contains multiple numerical features related to housing

Target variable: MedHouseVal

Suitable for linear, polynomial, and regularized regression

ğŸ§  Regression Models Implemented
1ï¸âƒ£ Linear Regression

Baseline model

Uses multiple features to predict housing prices

2ï¸âƒ£ Polynomial Regression

Captures non-linear relationships

Higher model complexity increases risk of overfitting

3ï¸âƒ£ Ridge Regression (L2 Regularization)

Penalizes large coefficients

Reduces model variance

Retains all features

4ï¸âƒ£ Lasso Regression (L1 Regularization)

Shrinks some coefficients to exactly zero

Performs automatic feature selection

5ï¸âƒ£ Elastic Net Regression

Combines L1 and L2 penalties

Useful when features are highly correlated

âš™ï¸ Methodology

Data loading and preprocessing

Trainâ€“test split

Feature scaling using StandardScaler

Model training and evaluation

Hyperparameter analysis (alpha)

Visualization of coefficients and errors

ğŸ“ Evaluation Metrics

RÂ² Score â€“ goodness of fit

RMSE â€“ prediction error magnitude

ğŸ“Š Key Results & Observations

Polynomial regression improves fit but may overfit

Ridge regression stabilizes coefficients and improves generalization

Lasso regression removes less important features

Elastic Net balances sparsity and stability

ğŸ“ˆ Visual Analysis

Coefficient comparison across models

Alpha (Î») vs RMSE curves for Ridge and Lasso

Clear visualization of regularization effects

ğŸ§  Key Learnings

Regularization is essential for controlling model complexity

Feature scaling is mandatory for regularized models

Model performance alone is not enough â€” interpretability matters

ğŸ› ï¸ Technologies Used

Python

NumPy

Pandas

Matplotlib

Scikit-learn

ğŸš€ Next Steps

Extend the project to classification models

Add Logistic and Softmax Regression

Compare binary vs multiclass classification strategies

ğŸ“Œ Author

Yashi
B.Tech â€“ AI & Data Science
Aspiring Machine Learning Engineer
